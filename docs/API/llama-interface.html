<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>LlamaInterface - OhMyDialogSystem Wiki</title>
  <link rel="stylesheet" href="../styles.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
</head>
<body>
  <canvas id="neural-bg"></canvas>

  <div class="wiki-container">
    <nav class="sidebar">
      <div class="sidebar-header">
        <div class="sidebar-logo-icon">üß†</div>
        <div class="sidebar-logo">OhMyDialogSystem</div>
        <div class="sidebar-subtitle">AI-Powered Dialogues</div>
      </div>

      <div class="nav-section">
        <div class="nav-section-title">General</div>
        <ul class="nav-links">
          <li><a href="../index.html"><span class="icon">üè†</span> Home</a></li>
        </ul>
      </div>

      <div class="nav-section">
        <div class="nav-section-title">Getting Started</div>
        <ul class="nav-links">
          <li><a href="../Guides/quickstart.html"><span class="icon">üöÄ</span> Quick Start</a></li>
          <li><a href="../Guides/installation.html"><span class="icon">üì¶</span> Installation</a></li>
          <li><a href="../Guides/first-npc.html"><span class="icon">üéÆ</span> Your First NPC</a></li>
        </ul>
      </div>

      <div class="nav-section">
        <div class="nav-section-title">API Reference</div>
        <ul class="nav-links">
          <li><a href="index.html"><span class="icon">üìö</span> Overview</a></li>
          <li><a href="llama-interface.html" class="active"><span class="icon">ü¶ô</span> LlamaInterface</a></li>
          <li><a href="dialogue-manager.html"><span class="icon">üí¨</span> DialogueManager</a></li>
          <li><a href="memory-manager.html"><span class="icon">üß†</span> MemoryManager</a></li>
          <li><a href="tts-manager.html"><span class="icon">üîä</span> TTSManager</a></li>
          <li><a href="resources.html"><span class="icon">üìÑ</span> Resources</a></li>
        </ul>
      </div>

      <div class="nav-section">
        <div class="nav-section-title">Technical</div>
        <ul class="nav-links">
          <li><a href="../Technical/index.html"><span class="icon">‚öôÔ∏è</span> Overview</a></li>
          <li><a href="../Technical/architecture.html"><span class="icon">üèóÔ∏è</span> Architecture</a></li>
          <li><a href="../Technical/gdextension.html"><span class="icon">üîß</span> GDExtension Build</a></li>
        </ul>
      </div>

      <div class="nav-section">
        <div class="nav-section-title">External Links</div>
        <ul class="nav-links">
          <li><a href="https://github.com/lobinuxsoft/OhMyDialogSystem" target="_blank"><span class="icon">üîó</span> GitHub Repository</a></li>
          <li><a href="https://github.com/users/lobinuxsoft/projects/5" target="_blank"><span class="icon">üìã</span> Project Board</a></li>
        </ul>
      </div>
    </nav>

    <main class="main-content">
      <div class="breadcrumbs">
        <a href="../index.html">Wiki</a> <span>/</span> <a href="index.html">API Reference</a> <span>/</span> <span>LlamaInterface</span>
      </div>

      <h1>ü¶ô LlamaInterface</h1>

      <p><span class="badge badge-cyan">GDExtension</span> <span class="badge badge-purple">C++</span></p>

      <p>Interfaz de bajo nivel para <a href="https://github.com/ggerganov/llama.cpp" target="_blank">llama.cpp</a>. Permite cargar modelos GGUF y gestionar su ciclo de vida.</p>

      <blockquote>
        <strong>Nota:</strong> Esta clase es parte del m√≥dulo GDExtension nativo. Requiere que el addon est√© correctamente compilado para tu plataforma.
      </blockquote>

      <hr>

      <h2>Herencia</h2>

      <p><code>RefCounted</code> ‚Üí <code>LlamaInterface</code></p>

      <hr>

      <h2>Descripci√≥n</h2>

      <p><code>LlamaInterface</code> proporciona una API de alto nivel para cargar modelos de lenguaje en formato GGUF usando la librer√≠a llama.cpp. Esta clase maneja la inicializaci√≥n del backend, carga de modelos, gesti√≥n de memoria y contexto de inferencia.</p>

      <p>Es la base sobre la cual se construyen los componentes de m√°s alto nivel como <code>DialogueManager</code>.</p>

      <hr>

      <h2>M√©todos</h2>

      <table>
        <thead>
          <tr>
            <th>Retorno</th>
            <th>M√©todo</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><code>Error</code></td>
            <td><a href="#load_model">load_model</a>(path: String, params: Dictionary = {})</td>
          </tr>
          <tr>
            <td><code>void</code></td>
            <td><a href="#unload_model">unload_model</a>()</td>
          </tr>
          <tr>
            <td><code>bool</code></td>
            <td><a href="#is_model_loaded">is_model_loaded</a>() const</td>
          </tr>
          <tr>
            <td><code>Dictionary</code></td>
            <td><a href="#get_model_info">get_model_info</a>() const</td>
          </tr>
          <tr>
            <td><code>String</code></td>
            <td><a href="#get_model_path">get_model_path</a>() const</td>
          </tr>
        </tbody>
      </table>

      <h3>Generaci√≥n de Texto</h3>

      <table>
        <thead>
          <tr>
            <th>Retorno</th>
            <th>M√©todo</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><code>String</code></td>
            <td><a href="#generate">generate</a>(prompt: String)</td>
          </tr>
        </tbody>
      </table>

      <h3>Par√°metros de Sampling</h3>

      <table>
        <thead>
          <tr>
            <th>Retorno</th>
            <th>M√©todo</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><code>void</code></td>
            <td><a href="#set_temperature">set_temperature</a>(temperature: float)</td>
          </tr>
          <tr>
            <td><code>float</code></td>
            <td><a href="#get_temperature">get_temperature</a>() const</td>
          </tr>
          <tr>
            <td><code>void</code></td>
            <td><a href="#set_top_p">set_top_p</a>(top_p: float)</td>
          </tr>
          <tr>
            <td><code>float</code></td>
            <td><a href="#get_top_p">get_top_p</a>() const</td>
          </tr>
          <tr>
            <td><code>void</code></td>
            <td><a href="#set_top_k">set_top_k</a>(top_k: int)</td>
          </tr>
          <tr>
            <td><code>int</code></td>
            <td><a href="#get_top_k">get_top_k</a>() const</td>
          </tr>
          <tr>
            <td><code>void</code></td>
            <td><a href="#set_max_tokens">set_max_tokens</a>(max_tokens: int)</td>
          </tr>
          <tr>
            <td><code>int</code></td>
            <td><a href="#get_max_tokens">get_max_tokens</a>() const</td>
          </tr>
          <tr>
            <td><code>void</code></td>
            <td><a href="#set_repeat_penalty">set_repeat_penalty</a>(penalty: float)</td>
          </tr>
          <tr>
            <td><code>float</code></td>
            <td><a href="#get_repeat_penalty">get_repeat_penalty</a>() const</td>
          </tr>
          <tr>
            <td><code>void</code></td>
            <td><a href="#set_min_p">set_min_p</a>(min_p: float)</td>
          </tr>
          <tr>
            <td><code>float</code></td>
            <td><a href="#get_min_p">get_min_p</a>() const</td>
          </tr>
          <tr>
            <td><code>void</code></td>
            <td><a href="#set_seed">set_seed</a>(seed: int)</td>
          </tr>
          <tr>
            <td><code>int</code></td>
            <td><a href="#get_seed">get_seed</a>() const</td>
          </tr>
        </tbody>
      </table>

      <h3>Stop Sequences</h3>

      <table>
        <thead>
          <tr>
            <th>Retorno</th>
            <th>M√©todo</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><code>void</code></td>
            <td><a href="#set_stop_sequences">set_stop_sequences</a>(sequences: PackedStringArray)</td>
          </tr>
          <tr>
            <td><code>PackedStringArray</code></td>
            <td><a href="#get_stop_sequences">get_stop_sequences</a>() const</td>
          </tr>
          <tr>
            <td><code>void</code></td>
            <td><a href="#clear_stop_sequences">clear_stop_sequences</a>()</td>
          </tr>
        </tbody>
      </table>

      <h3>Timeout</h3>

      <table>
        <thead>
          <tr>
            <th>Retorno</th>
            <th>M√©todo</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><code>void</code></td>
            <td><a href="#set_timeout">set_timeout</a>(timeout_ms: int)</td>
          </tr>
          <tr>
            <td><code>int</code></td>
            <td><a href="#get_timeout">get_timeout</a>() const</td>
          </tr>
          <tr>
            <td><code>bool</code></td>
            <td><a href="#has_generation_timed_out">has_generation_timed_out</a>() const</td>
          </tr>
        </tbody>
      </table>

      <hr>

      <h2>Descripciones de M√©todos</h2>

      <h3 id="load_model">load_model</h3>

      <pre data-lang="GDSCRIPT"><code>Error load_model(path: String, params: Dictionary = {})</code></pre>

      <p>Carga un modelo GGUF desde la ruta especificada.</p>

      <h4>Par√°metros</h4>

      <table>
        <thead>
          <tr>
            <th>Par√°metro</th>
            <th>Tipo</th>
            <th>Descripci√≥n</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><code>path</code></td>
            <td>String</td>
            <td>Ruta al archivo .gguf. Soporta <code>res://</code> y <code>user://</code></td>
          </tr>
          <tr>
            <td><code>params</code></td>
            <td>Dictionary</td>
            <td>Par√°metros opcionales de configuraci√≥n</td>
          </tr>
        </tbody>
      </table>

      <h4>Par√°metros del Dictionary</h4>

      <table>
        <thead>
          <tr>
            <th>Key</th>
            <th>Tipo</th>
            <th>Default</th>
            <th>Descripci√≥n</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><code>n_ctx</code></td>
            <td>int</td>
            <td>modelo</td>
            <td>Tama√±o de contexto en tokens</td>
          </tr>
          <tr>
            <td><code>n_gpu_layers</code></td>
            <td>int</td>
            <td>0</td>
            <td>Capas a offload a GPU (0 = solo CPU)</td>
          </tr>
          <tr>
            <td><code>n_batch</code></td>
            <td>int</td>
            <td>512</td>
            <td>Tama√±o de batch para procesamiento</td>
          </tr>
          <tr>
            <td><code>n_threads</code></td>
            <td>int</td>
            <td>auto</td>
            <td>Threads para generaci√≥n</td>
          </tr>
          <tr>
            <td><code>n_threads_batch</code></td>
            <td>int</td>
            <td>auto</td>
            <td>Threads para procesamiento de batch</td>
          </tr>
          <tr>
            <td><code>use_mmap</code></td>
            <td>bool</td>
            <td>true</td>
            <td>Usar memory-mapped file</td>
          </tr>
          <tr>
            <td><code>use_mlock</code></td>
            <td>bool</td>
            <td>false</td>
            <td>Bloquear modelo en RAM</td>
          </tr>
          <tr>
            <td><code>vocab_only</code></td>
            <td>bool</td>
            <td>false</td>
            <td>Solo cargar vocabulario (sin pesos)</td>
          </tr>
        </tbody>
      </table>

      <h4>Retorno</h4>

      <p>Retorna <code>OK</code> si el modelo se carg√≥ correctamente, o un c√≥digo de error:</p>

      <ul>
        <li><code>ERR_FILE_NOT_FOUND</code> - El archivo no existe</li>
        <li><code>ERR_CANT_OPEN</code> - No se pudo abrir/parsear el modelo</li>
        <li><code>ERR_CANT_CREATE</code> - No se pudo crear el contexto</li>
      </ul>

      <h4>Ejemplo</h4>

      <pre data-lang="GDSCRIPT"><code>var llama = LlamaInterface.new()
var err = llama.load_model("user://models/mistral-7b-q4.gguf", {
    "n_ctx": 4096,
    "n_gpu_layers": 0
})

if err != OK:
    push_error("Failed to load model: " + str(err))
    return

print("Model loaded successfully!")</code></pre>

      <hr>

      <h3 id="unload_model">unload_model</h3>

      <pre data-lang="GDSCRIPT"><code>void unload_model()</code></pre>

      <p>Descarga el modelo actual y libera todos los recursos asociados (contexto, backend, memoria).</p>

      <p>Es seguro llamar este m√©todo incluso si no hay modelo cargado.</p>

      <h4>Ejemplo</h4>

      <pre data-lang="GDSCRIPT"><code>llama.unload_model()
print("Model unloaded")</code></pre>

      <hr>

      <h3 id="is_model_loaded">is_model_loaded</h3>

      <pre data-lang="GDSCRIPT"><code>bool is_model_loaded() const</code></pre>

      <p>Verifica si hay un modelo cargado y listo para usar.</p>

      <h4>Retorno</h4>

      <p><code>true</code> si hay un modelo cargado, <code>false</code> en caso contrario.</p>

      <h4>Ejemplo</h4>

      <pre data-lang="GDSCRIPT"><code>if llama.is_model_loaded():
    print("Ready to generate!")
else:
    print("No model loaded")</code></pre>

      <hr>

      <h3 id="get_model_info">get_model_info</h3>

      <pre data-lang="GDSCRIPT"><code>Dictionary get_model_info() const</code></pre>

      <p>Retorna informaci√≥n detallada sobre el modelo cargado.</p>

      <h4>Retorno</h4>

      <p>Un Dictionary con las siguientes keys (vac√≠o si no hay modelo cargado):</p>

      <table>
        <thead>
          <tr>
            <th>Key</th>
            <th>Tipo</th>
            <th>Descripci√≥n</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><code>description</code></td>
            <td>String</td>
            <td>Descripci√≥n del modelo</td>
          </tr>
          <tr>
            <td><code>path</code></td>
            <td>String</td>
            <td>Ruta del archivo cargado</td>
          </tr>
          <tr>
            <td><code>size_bytes</code></td>
            <td>int</td>
            <td>Tama√±o del modelo en bytes</td>
          </tr>
          <tr>
            <td><code>n_params</code></td>
            <td>int</td>
            <td>N√∫mero de par√°metros</td>
          </tr>
          <tr>
            <td><code>n_ctx_train</code></td>
            <td>int</td>
            <td>Contexto de entrenamiento</td>
          </tr>
          <tr>
            <td><code>n_embd</code></td>
            <td>int</td>
            <td>Dimensi√≥n de embeddings</td>
          </tr>
          <tr>
            <td><code>n_layer</code></td>
            <td>int</td>
            <td>N√∫mero de capas</td>
          </tr>
          <tr>
            <td><code>n_head</code></td>
            <td>int</td>
            <td>N√∫mero de attention heads</td>
          </tr>
          <tr>
            <td><code>n_ctx</code></td>
            <td>int</td>
            <td>Contexto actual configurado</td>
          </tr>
          <tr>
            <td><code>n_batch</code></td>
            <td>int</td>
            <td>Batch size actual</td>
          </tr>
          <tr>
            <td><code>vocab_size</code></td>
            <td>int</td>
            <td>Tama√±o del vocabulario</td>
          </tr>
          <tr>
            <td><code>vocab_type</code></td>
            <td>int</td>
            <td>Tipo de tokenizador (0=SPM, 1=BPE, etc.)</td>
          </tr>
          <tr>
            <td><code>bos_token</code></td>
            <td>int</td>
            <td>ID del token BOS</td>
          </tr>
          <tr>
            <td><code>eos_token</code></td>
            <td>int</td>
            <td>ID del token EOS</td>
          </tr>
          <tr>
            <td><code>has_encoder</code></td>
            <td>bool</td>
            <td>Si el modelo tiene encoder</td>
          </tr>
          <tr>
            <td><code>has_decoder</code></td>
            <td>bool</td>
            <td>Si el modelo tiene decoder</td>
          </tr>
          <tr>
            <td><code>is_recurrent</code></td>
            <td>bool</td>
            <td>Si es recurrente (Mamba, RWKV)</td>
          </tr>
          <tr>
            <td><code>rope_type</code></td>
            <td>int</td>
            <td>Tipo de RoPE</td>
          </tr>
        </tbody>
      </table>

      <h4>Ejemplo</h4>

      <pre data-lang="GDSCRIPT"><code>var info = llama.get_model_info()
if not info.is_empty():
    print("Model: ", info.description)
    print("Parameters: ", info.n_params / 1_000_000_000.0, "B")
    print("Context: ", info.n_ctx, " tokens")
    print("Layers: ", info.n_layer)</code></pre>

      <hr>

      <h3 id="get_model_path">get_model_path</h3>

      <pre data-lang="GDSCRIPT"><code>String get_model_path() const</code></pre>

      <p>Retorna la ruta del modelo actualmente cargado.</p>

      <h4>Retorno</h4>

      <p>La ruta del modelo, o un String vac√≠o si no hay modelo cargado.</p>

      <hr>

      <h2>Generaci√≥n de Texto</h2>

      <h3 id="generate">generate</h3>

      <pre data-lang="GDSCRIPT"><code>String generate(prompt: String)</code></pre>

      <p>Genera texto de forma s√≠ncrona (bloqueante) a partir de un prompt.</p>

      <h4>Par√°metros</h4>

      <table>
        <thead>
          <tr>
            <th>Par√°metro</th>
            <th>Tipo</th>
            <th>Descripci√≥n</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><code>prompt</code></td>
            <td>String</td>
            <td>El texto de entrada para continuar</td>
          </tr>
        </tbody>
      </table>

      <h4>Retorno</h4>

      <p>El texto generado, o un String vac√≠o si hay error o no hay modelo cargado.</p>

      <h4>Ejemplo</h4>

      <pre data-lang="GDSCRIPT"><code>var llama = LlamaInterface.new()
llama.load_model("res://models/mistral-7b-q4.gguf")

# Configurar par√°metros
llama.set_temperature(0.7)
llama.set_max_tokens(100)

# Generar texto
var response = llama.generate("The quick brown fox")
print(response)  # " jumps over the lazy dog..."</code></pre>

      <blockquote>
        <strong>Nota:</strong> Esta operaci√≥n es bloqueante. Para generaci√≥n as√≠ncrona, usa un Thread o espera la implementaci√≥n de <code>generate_async()</code>.
      </blockquote>

      <hr>

      <h2>Par√°metros de Sampling</h2>

      <h3 id="set_temperature">set_temperature / get_temperature</h3>

      <pre data-lang="GDSCRIPT"><code>void set_temperature(temperature: float)
float get_temperature() const</code></pre>

      <p>Controla la aleatoriedad de la generaci√≥n.</p>

      <ul>
        <li><strong>0.0</strong> = Greedy (siempre elige el token m√°s probable)</li>
        <li><strong>0.7-0.9</strong> = Balance creatividad/coherencia (recomendado)</li>
        <li><strong>1.0+</strong> = M√°s aleatorio/creativo</li>
      </ul>

      <p><strong>Default:</strong> <code>0.8</code></p>

      <hr>

      <h3 id="set_top_p">set_top_p / get_top_p</h3>

      <pre data-lang="GDSCRIPT"><code>void set_top_p(top_p: float)
float get_top_p() const</code></pre>

      <p>Nucleus sampling - considera solo tokens cuya probabilidad acumulada sea ‚â§ top_p.</p>

      <p><strong>Default:</strong> <code>0.95</code> | <strong>Rango:</strong> <code>0.0 - 1.0</code></p>

      <hr>

      <h3 id="set_top_k">set_top_k / get_top_k</h3>

      <pre data-lang="GDSCRIPT"><code>void set_top_k(top_k: int)
int get_top_k() const</code></pre>

      <p>Considera solo los top-k tokens m√°s probables. <code>0</code> = desactivado.</p>

      <p><strong>Default:</strong> <code>40</code></p>

      <hr>

      <h3 id="set_max_tokens">set_max_tokens / get_max_tokens</h3>

      <pre data-lang="GDSCRIPT"><code>void set_max_tokens(max_tokens: int)
int get_max_tokens() const</code></pre>

      <p>N√∫mero m√°ximo de tokens a generar. M√≠nimo: <code>1</code>.</p>

      <p><strong>Default:</strong> <code>256</code></p>

      <hr>

      <h3 id="set_repeat_penalty">set_repeat_penalty / get_repeat_penalty</h3>

      <pre data-lang="GDSCRIPT"><code>void set_repeat_penalty(penalty: float)
float get_repeat_penalty() const</code></pre>

      <p>Penaliza la repetici√≥n de tokens. <code>1.0</code> = sin penalizaci√≥n.</p>

      <p><strong>Default:</strong> <code>1.1</code> | <strong>Rango:</strong> <code>1.0 - 2.0</code></p>

      <hr>

      <h3 id="set_min_p">set_min_p / get_min_p</h3>

      <pre data-lang="GDSCRIPT"><code>void set_min_p(min_p: float)
float get_min_p() const</code></pre>

      <p>Filtra tokens con probabilidad menor a min_p * prob_max.</p>

      <p><strong>Default:</strong> <code>0.05</code> | <strong>Rango:</strong> <code>0.0 - 1.0</code></p>

      <hr>

      <h3 id="set_seed">set_seed / get_seed</h3>

      <pre data-lang="GDSCRIPT"><code>void set_seed(seed: int)
int get_seed() const</code></pre>

      <p>Semilla para reproducibilidad. <code>0xFFFFFFFF</code> = aleatorio.</p>

      <p><strong>Default:</strong> <code>0xFFFFFFFF</code> (aleatorio)</p>

      <hr>

      <h2>Stop Sequences</h2>

      <h3 id="set_stop_sequences">set_stop_sequences</h3>

      <pre data-lang="GDSCRIPT"><code>void set_stop_sequences(sequences: PackedStringArray)</code></pre>

      <p>Define secuencias que detienen la generaci√≥n cuando se encuentran.</p>

      <h4>Ejemplo</h4>

      <pre data-lang="GDSCRIPT"><code># Detener en fin de turno de di√°logo
llama.set_stop_sequences(["User:", "\n\n", "[END]"])</code></pre>

      <hr>

      <h3 id="get_stop_sequences">get_stop_sequences</h3>

      <pre data-lang="GDSCRIPT"><code>PackedStringArray get_stop_sequences() const</code></pre>

      <p>Retorna las secuencias de stop configuradas.</p>

      <hr>

      <h3 id="clear_stop_sequences">clear_stop_sequences</h3>

      <pre data-lang="GDSCRIPT"><code>void clear_stop_sequences()</code></pre>

      <p>Elimina todas las secuencias de stop.</p>

      <hr>

      <h2>Timeout</h2>

      <h3 id="set_timeout">set_timeout / get_timeout</h3>

      <pre data-lang="GDSCRIPT"><code>void set_timeout(timeout_ms: int)
int get_timeout() const</code></pre>

      <p>Configura el timeout m√°ximo para la generaci√≥n en milisegundos.</p>

      <ul>
        <li><strong>0</strong> = Sin timeout (espera indefinidamente)</li>
        <li><strong>>0</strong> = Tiempo m√°ximo en ms</li>
      </ul>

      <p><strong>Default:</strong> <code>0</code> (sin timeout)</p>

      <h4>Ejemplo</h4>

      <pre data-lang="GDSCRIPT"><code># Limitar generaci√≥n a 10 segundos
llama.set_timeout(10000)

var response = llama.generate("Tell me a story")

if llama.has_generation_timed_out():
    print("Generation timed out!")
    print("Partial response: ", response)</code></pre>

      <hr>

      <h3 id="has_generation_timed_out">has_generation_timed_out</h3>

      <pre data-lang="GDSCRIPT"><code>bool has_generation_timed_out() const</code></pre>

      <p>Verifica si la √∫ltima generaci√≥n termin√≥ por timeout.</p>

      <h4>Retorno</h4>

      <p><code>true</code> si la √∫ltima llamada a <code>generate()</code> termin√≥ por timeout.</p>

      <hr>

      <h2>Se√±ales</h2>

      <table>
        <thead>
          <tr>
            <th>Se√±al</th>
            <th>Descripci√≥n</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><code>generation_timeout()</code></td>
            <td>Emitida cuando la generaci√≥n excede el tiempo l√≠mite configurado</td>
          </tr>
        </tbody>
      </table>

      <h4>Ejemplo</h4>

      <pre data-lang="GDSCRIPT"><code>func _ready():
    llama = LlamaInterface.new()
    llama.generation_timeout.connect(_on_timeout)

func _on_timeout():
    print("Generation took too long!")</code></pre>

      <hr>

      <h2>Ejemplo Completo</h2>

      <pre data-lang="GDSCRIPT"><code>extends Node

var llama: LlamaInterface

func _ready():
    llama = LlamaInterface.new()
    llama.generation_timeout.connect(_on_generation_timeout)

    # Cargar modelo con configuraci√≥n personalizada
    var err = llama.load_model("user://models/qwen2.5-7b-q4_k_m.gguf", {
        "n_ctx": 8192,
        "n_gpu_layers": 35,  # Offload a GPU si est√° disponible
        "n_threads": 8
    })

    if err != OK:
        push_error("Error loading model")
        return

    # Mostrar informaci√≥n del modelo
    var info = llama.get_model_info()
    print("=== Model Info ===")
    print("Description: ", info.get("description", "Unknown"))
    print("Size: ", info.get("size_bytes", 0) / (1024 * 1024), " MB")
    print("Parameters: ", info.get("n_params", 0))

    # Configurar generaci√≥n
    llama.set_temperature(0.7)
    llama.set_max_tokens(150)
    llama.set_timeout(30000)  # 30 segundos m√°ximo
    llama.set_stop_sequences(["User:", "\n\n"])

    # Generar respuesta
    var response = llama.generate("Hello! How are you today?")

    if llama.has_generation_timed_out():
        print("Warning: Generation timed out")
        print("Partial response: ", response)
    else:
        print("Response: ", response)

func _on_generation_timeout():
    print("Generation exceeded time limit!")

func _exit_tree():
    if llama and llama.is_model_loaded():
        llama.unload_model()</code></pre>

      <hr>

      <h2>Modelos Compatibles</h2>

      <p>LlamaInterface soporta cualquier modelo en formato <strong>GGUF</strong>. Algunos modelos recomendados:</p>

      <table>
        <thead>
          <tr>
            <th>Modelo</th>
            <th>Tama√±o</th>
            <th>VRAM (Q4)</th>
            <th>Uso Recomendado</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Qwen2.5-3B</td>
            <td>~2 GB</td>
            <td>~3 GB</td>
            <td>NPCs simples, respuestas cortas</td>
          </tr>
          <tr>
            <td>Mistral-7B</td>
            <td>~4 GB</td>
            <td>~6 GB</td>
            <td>Balance calidad/velocidad</td>
          </tr>
          <tr>
            <td>Qwen2.5-7B</td>
            <td>~4 GB</td>
            <td>~6 GB</td>
            <td>Di√°logos complejos</td>
          </tr>
          <tr>
            <td>Llama-3.1-8B</td>
            <td>~5 GB</td>
            <td>~7 GB</td>
            <td>Alta calidad general</td>
          </tr>
        </tbody>
      </table>

      <blockquote>
        <strong>Tip:</strong> Usa cuantizaciones Q4_K_M o Q5_K_M para el mejor balance entre calidad y rendimiento.
      </blockquote>

      <hr>

      <h2>Notas T√©cnicas</h2>

      <ul>
        <li>La clase inicializa autom√°ticamente el backend de llama.cpp al cargar un modelo</li>
        <li>Solo puede haber un modelo cargado a la vez. Cargar un nuevo modelo descarga el anterior</li>
        <li>El destructor libera autom√°ticamente todos los recursos</li>
        <li>Compatible con multi-threading via OpenMP</li>
        <li>Soporta GPU offloading con CUDA, Vulkan o Metal (seg√∫n compilaci√≥n)</li>
      </ul>

      <hr>

      <a href="index.html">‚Üê Volver a API Reference</a>

      <footer class="wiki-footer">
        <p>OhMyDialogSystem Wiki | <a href="https://github.com/lobinuxsoft/OhMyDialogSystem" target="_blank">GitHub</a></p>
      </footer>
    </main>
  </div>

  <script src="../ai-bg.js"></script>
</body>
</html>
